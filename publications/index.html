<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Gopi Krishna Erabati </title> <meta name="author" content="Gopi Krishna Erabati"> <meta name="description" content="A PhD candidate at Institute of Systems and Robotics, University of Coimbra, Portugal, focusing on Scene Understanding for Autonomous Driving. "> <meta name="keywords" content="Computer Vision, Autonomous Driving, Deel Learning, Scene Understanding"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/6.jpg?b2748a8c220f6715934bfddac3f6df51"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gopi-erabati.github.io/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Gopi Krishna</span> Erabati </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/experience/">Experience </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">Awards </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_Gopi.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-LiDAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/retformer-480.webp 480w,/assets/img/publication_preview/retformer-800.webp 800w,/assets/img/publication_preview/retformer-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/retformer.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="retformer.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="retformer" class="col-sm-8"> <div class="title">RetFormer: Embracing Point Cloud Transformer with Retentive Network</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>IEEE Transactions on Intelligent Vehicles</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10568420" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/mxJ7_OsqMiQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/RetFormer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Point Cloud Transformers (PCTs) have gained lot of attention not only on the indoor data but also on the large-scale outdoor 3D point clouds, such as in autonomous driving. However, the vanilla self-attention mechanism in PCTs does not include any explicit prior spatial information about the quantized voxels (or pillars). Recently, Retentive Network has gained attention in the natural language processing (NLP) domain due to its efficient modelling capability and remarkable performance, leveraged by the introduction of explicit decay mechanism which incorporates the distance related spatial prior knowledge into the model. As the NLP tasks are causal and one-dimensional in nature, the explicit decay is designed to be unidirectional and one-dimensional. However, the pillars in the Bird’s Eye View (BEV) space are two-dimensional without causal properties. In this work, we propose RetFormer model by introducing bidirectional and two-dimensional decay mechanism for pillars in PCT and design the novel Multi-Scale Retentive Self-Attention (MSReSA) module. The introduction of explicit bidirectional and two-dimensional decay incorporates the 2D spatial distance related prior information of pillars into the PCT which significantly improves the modelling capacity of RetFormer. We evaluate our method on large-scale Waymo and KITTI datasets. RetFormer not only achieves significant performance gain over of 2.3 mAP and 3.2 mAP over PCT-based SST and FlatFormer respectively, and 6.4 mAP over sparse convolutional-based CenterPoint for small object pedestrian category on Waymo Open Dataset, but also is efficient with 3.2x speedup over SST and runs in real-time at  69 FPS on a RTX 4090 GPU.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">retformer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RetFormer: Embracing Point Cloud Transformer with Retentive Network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Intelligent Vehicles}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-Fusion</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/srfdet3d-480.webp 480w,/assets/img/publication_preview/srfdet3d-800.webp 800w,/assets/img/publication_preview/srfdet3d-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/srfdet3d.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="srfdet3d.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="srfdet3d" class="col-sm-8"> <div class="title">SRFDet3D: Sparse Region Fusion based 3D Object Detection</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>Neurocomputing</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pdf.sciencedirectassets.com/271597/1-s2.0-S0925231224X00235/1-s2.0-S092523122400585X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCTNNlwz75MW1wq3pgkpoMyugISNB3I9%2BsS14Oao9kYRQIhAJ0o1rXFZn0Jbtif7E5x8QgSqpe0BxxKi957z7umaMCtKrMFCFsQBRoMMDU5MDAzNTQ2ODY1Igw6nmg5KcyFkEAhvA0qkAW3iVZqKqBQv0NjdVN2vSej1k0gNkQj9emw3Z6J2x9pxdMQKzuM89xbplMpslXNoDjmO9H9AJ0QZWMTbefdkEUwSskVpubTPgOT%2FY5M17XEc8xxDC9W1%2FlygwK5oZqtpSbY8zuhsNGfRnAZdgXU2ouAxfbL726M2j%2FYXmu9gVXyi6x074x5E8TxNiwptxG%2BWe02SJM0F3%2BDqu%2BY6k7UyEEJsHZ118Zd1jMFJCGpBFOGCkOHH%2F%2BHHSd4Wr8vgqlqLlx07VMXX37EsoQgebzZYfRDmRsgvO6O38fiKh3qo1f95MMIbtRIasA6TRJESl1LGcVFwHhZ6tGHoRqQW0O1W%2Bs8v2vFxnYUKX41kE%2FiTYvVnSGu3qJ08UNZtq8BuhCwTQYPLe%2FZcyFK6coIIcTdtBBCbRruz4zXEiuuN6nq2QZGeeU7EHww8e69qiBRlEXEqDK8QiG8fJb7TMVTe2%2Bu%2BWZz3dixQz87A1ZgNOnX%2BtR5YHlLoAOm6Ley7H49ixOf0nuagO7dMKz2aHFzH1u%2BUv%2BHMNj2RfL0Jfsvog6m%2B2In5Va%2BT5p4mgBw2Rt%2FL9kUoJnRNEQF%2Fgl9jaszEt5sByyhnkbDiTDkCvBWkgO3l1Aq2v9oXUiM2%2Brs2wMTB41cReNOgMfV2KQV2rY4fpcQAX1VwPlsKxx7i5BJCHNNrujTy7Kj4rpMeBXq8GrAL3G06PfJtZNfHm9g0VfVzWVFeV94X5tC4PMnDjDxVdcV2K85wCOJOTSWgPUqaws3STY0eNNp7XSl7Ee9yTtdN7H54Wy5P353zTwxQj1mjiOkTEXwP4CI9UajOWtHuh7hRrHS61oKmTlwkBb%2FHoxCYz6rEUC8I3X0cbCksejSD12RSlJYWjCribeyBjqwAWGcW66D%2BeTE8iElxKxNmeM3i9Vb4eI02Ujiya4AzOuDpjTm%2Fo2NgKq0QymubwCtYgyWA26ixNg3pmD%2F%2FmfnkXarWB%2BQTRRDDwpdYCRd7rVsezSV4oiysq8zogPdzxg5IvQpsV2PJd2KeBlN9zRJe5Tm7offqCnvztmcfe2Ghd4RIgC5DjLTlnKFZy0aclNtszWOlQ%2BEb0TBBzQf56y3JjUpb%2Be6zVsd8s9pBLhV98CL&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240522T113701Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=299&amp;X-Amz-Credential=ASIAQ3PHCVTY3XX57AWT%2F20240522%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=8fca4171ab6c16c40bea80c1525e495bd397297716c7e005d345454d071635d6&amp;hash=1004c7480a8be94f156089de54e8a9841ce22e1f136f5900e3db7e5b98460e62&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S092523122400585X&amp;tid=spdf-df21eaf1-98b7-42e3-8897-a440afea22b1&amp;sid=3c33f25858a0c2495e6b318-a4f5072dd0a8gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=16125d5759065c555505&amp;rr=887c830a5abe692c&amp;cc=pt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/2yNej0T4w0Q" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/SRFDet3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Unlike the earlier 3D object detection approaches that formulate hand-crafted dense (in thousands) object proposals by leveraging anchors on dense feature maps, we formulate np (in hundreds) number of learnable sparse object proposals to predict 3D bounding box parameters. The sparse proposals in our approach are not only learnt during training but also are input-dependent, so they represent better object candidates during inference. Leveraging the sparse proposals, we fuse only the sparse regions of multi-modal features and we propose Sparse Region Fusion based 3D object Detection (SRFDet3D) network with mainly three components: an encoder for feature extraction, a region proposal generation module for sparse input-dependent proposals and a decoder for multi-modal feature fusion and iterative refinement of object proposals. Additionally for optimal training, we formulate our sparse detector with many-to-one label assignment based on Optimal Transport Algorithm (OTA). We conduct extensive experiments and analysis on publicly available large-scale autonomous driving datasets: nuScenes, KITTI, and Waymo. Our LiDAR-only SRFDet3D-L network achieves 63.1 mAP and outperforms the state-of-the-art networks on the nuScenes dataset, surpassing the dense detectors on KITTI and Waymo datasets. Our LiDAR-Camera model SRFDet3D achieves 64.7 mAP with improvements over existing fusion methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">srfdet3d</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SRFDet3D: Sparse Region Fusion based 3D Object Detection}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neurocomputing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{593}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{127814}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0925-2312}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.neucom.2024.127814}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S092523122400585X}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{3D object detection, Fusion, Camera, LiDAR, Autonomous driving, Computer vision}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DSemSeg</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/retseg3d-480.webp 480w,/assets/img/publication_preview/retseg3d-800.webp 800w,/assets/img/publication_preview/retseg3d-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/retseg3d.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="retseg3d.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="retseg3d" class="col-sm-8"> <div class="title">RetSeg3D: Retention-based 3D semantic segmentation for autonomous driving</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>Computer Vision and Image Understanding</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1016/j.cviu.2024.104231" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/aBN-v0A_WfU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/RetSeg3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>LiDAR semantic segmentation is one of the crucial tasks for scene understanding in autonomous driving. Recent trends suggest that voxel- or fusion-based methods obtain improved performance. However, the fusion-based methods are computationally expensive. On the other hand, the voxel-based methods uniformly employ local operators (e.g., 3D SparseConv) without considering the varying-density property of LiDAR point clouds, which result in inferior performance, specifically on far away sparse points due to limited receptive field. To tackle this issue, we propose novel retention block to capture long-range dependencies, maintain the receptive field of far away sparse points and design RetSeg3D, a retention-based 3D semantic segmentation model for autonomous driving. Instead of vanilla attention mechanism to model long-range dependencies, inspired by RetNet, we design cubic window multi-scale retentive self-attention (CW-MSRetSA) module with bidirectional and 3D explicit decay mechanism to introduce 3D spatial distance related prior information into the model to improve not only the receptive field but also the model capacity. Our novel retention block maintains the receptive field which significantly improve the performance of far away sparse points. We conduct extensive experiments and analysis on three large-scale datasets: SemanticKITTI, nuScenes and Waymo. Our method not only outperforms existing methods on far away sparse points but also on close and medium distance points and efficiently runs in real time at 52.1 FPS on a RTX 4090 GPU.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">retseg3d</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RetSeg3D: Retention-based 3D semantic segmentation for autonomous driving}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Image Understanding}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{104231}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1077-3142}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.cviu.2024.104231}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S1077314224003126}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{3D semantic segmentation, Retention, Autonomous driving, LiDAR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-LiDAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/delivotr-480.webp 480w,/assets/img/publication_preview/delivotr-800.webp 800w,/assets/img/publication_preview/delivotr-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/delivotr.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="delivotr.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="delivotr" class="col-sm-8"> <div class="title">DeLiVoTr: Deep and light-weight voxel transformer for 3D object detection</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>Intelligent Systems with Applications</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pdf.sciencedirectassets.com/779210/1-s2.0-S2667305324X00022/1-s2.0-S2667305324000371/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDC4paLsuxn5gKaiguKnMIN8IiGWd3FfoKIgFW59uORxgIhAJdfOmTzSlGX3SDaeFwBNTURUTIZaksRFVt%2BtaLYOR19KrMFCFwQBRoMMDU5MDAzNTQ2ODY1IgxoHV7td%2Fpo%2B0F38AYqkAXdtX3j8RTlaZ%2B15j%2ByqoMLNtLkXGV%2BUKAMggrNdZFfi4%2BiPRE5H%2FO8tseYT2w60P%2BtPaisTvKa3irfdMCPrOPWOqDRCy2Y8LZMxckNs9VeHHmsyKV%2Bz86N%2BCHwq3w%2FYiVVMtjNia5IIzyQDVvpEepAmpS3kOCHepsNsNGBGKUM2bnLf5RTBNc%2BPIxCOp4L97d2M2sG1s5zxq59WBOX1rTiWAP40X9dwNXMLnGHPmdSbPfJu0Z%2FGk6eqJ5rB0s5c6JaZY6r8r2R0l1MfGurywU%2FeUD7hMQ4UK8fnTcm1rfBQ%2BbZ8owFO8bVojVj81D4OJtZIp4qnxNrrAfT4bVkWyiU7rOrj%2Fs1jtrogLutxJdyztZWORIq3UUXOV3PKlZl7o1VYWD7%2FJvtWLbPr9MXyhV4llqP2VGSSz3HvFwnhWvzs9kMrOkUh4tqRfz8iA89kXTWwYrK3LFLij9X9RySub11f%2FgPanG6gWGdI68XV54uqrWINkbhuawqstWEYK%2FYSEhQS5x9t7MlLAOoTAIOvUfi7JT6a6JmDvoHQ6x3hSrABvMH9uED5JCeI3jGdwU1IYpr61HaaSkaw%2FJ6Ly3gky8tnsY7wSBFRQ5mTcru0CbaNWcmZXuuOzGRsYAkelcLdq%2FLECnPtdeuh5603YkRteiCwJFGkYNyY3PoGKRjj0Ypg%2F9C6%2FZtuGSs8DEbR6St8bTkGvo7kJ8LvwY6zmPpCBCtPfNvS9hfpNsUu6u0IDfoMebNRRXK57vayu%2FKuvXrhnwl1yp41mus8mEwsQHPPoTC40J1oFBFFvDQcuAPANvEPKMKrjzQXNoQmZrRnbEjfoXmxGFiW%2FxkGGPpMNoz4sjYflHd7dqgwg86DzDJDHVeQTDglbeyBjqwASZCLgA06JQLcFqlK5%2F4uRHG6ov0Gd%2BcYInT94m39H6tbwlQq5pe2n93jAfBWiDMg7y8wmDl%2BCkSA7xuJ%2BYErOGr2nXp3fZpL6nh%2F9aGBkq1UAJwkjdL0F%2B5Mnug1WHezdXPkz0928K3ePgEkEpFd0iAN6rXBBdhidB2%2BWzuymccNK1C6GNdOw%2BZPxXkJ%2B3%2FBDxE805zFvILW0tuojPHkCwep3bYNVOV8%2BNJIguuVDvg&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240522T113500Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYQIIHXXFU%2F20240522%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=1a25a67ed6846623b8ebeb4f2ca0e10a1909c1f60d3f6d95d17b28039248eb38&amp;hash=b8529fc7651712323c76ea6ed60ae39394386f272b03e534d696b598847d5c03&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S2667305324000371&amp;tid=spdf-51c0fe81-e383-4769-925d-bcfef88d7bd2&amp;sid=3c33f25858a0c2495e6b318-a4f5072dd0a8gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=16125d5759065c565451&amp;rr=887c80156c2f692c&amp;cc=pt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/jnYDfZbVsjw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/DeLiVoTr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The image-based backbone (feature extraction) networks downsample the feature maps not only to increase the receptive field but also to efficiently detect objects of various scales. The existing feature extraction networks in LiDAR-based 3D object detection tasks follow the feature map downsampling similar to image-based feature extraction networks to increase the receptive field. But, such downsampling of LiDAR feature maps in large-scale autonomous driving scenarios hinder the detection of small size objects, such as pedestrians. To solve this issue we design an architecture that not only maintains the same scale of the feature maps but also the receptive field in the feature extraction network to aid for efficient detection of small size objects. We resort to attention mechanism to build sufficient receptive field and we propose a Deep and Light-weight Voxel Transformer (DeLiVoTr) network with voxel intra- and inter-region transformer modules to extract voxel local and global features respectively. We introduce DeLiVoTr block that uses transformations with expand and reduce strategy to vary the width and depth of the network efficiently. This facilitates to learn wider and deeper voxel representations and enables to use not only smaller dimension for attention mechanism but also a light-weight feed-forward network, facilitating the reduction of parameters and operations. In addition to model scaling, we employ layer-level scaling of DeLiVoTr encoder layers for efficient parameter allocation in each encoder layer instead of fixed number of parameters as in existing approaches. Leveraging layer-level depth and width scaling we formulate three variants of DeLiVoTr network. We conduct extensive experiments and analysis on large-scale Waymo and KITTI datasets. Our network surpasses state-of-the-art methods for detection of small objects (pedestrians) with an inference speed of 20.5 FPS.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">delivotr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DeLiVoTr: Deep and light-weight voxel transformer for 3D object detection}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Intelligent Systems with Applications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{200361}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2667-3053}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.iswa.2024.200361}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S2667305324000371}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{3D object detection, Transformer, Voxel, LiDAR, Autonomous driving, Computer vision}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-LiDAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ddet3d-480.webp 480w,/assets/img/publication_preview/ddet3d-800.webp 800w,/assets/img/publication_preview/ddet3d-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ddet3d.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ddet3d.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ddet3d" class="col-sm-8"> <div class="title">DDet3D: Embracing 3D Object Detector with Diffusion</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>Applied Intelligence</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/3XQd5G-wGNM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/DDet3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Existing approaches rely on heuristic or learnable object proposals (which are required to be optimised during training) for 3D object detection. In our approach, we replace the hand-crafted or learnable object proposals with randomly generated object proposals by formulating a new paradigm to employ a diffusion model to detect 3D objects from a set of randomly generated and supervised learning-based object proposals in an autonomous driving application. We propose DDet3D, a diffusion-based 3D object detection framework that formulates 3D object detection as a generative task over the 3D bounding box coordinates in 3D space. To our knowledge, this work is the first to formulate the 3D object detection with denoising diffusion model and to establish that 3D randomly generated and supervised learning-based proposals (different from empirical anchors or learnt queries) are also potential object candidates for 3D object detection. During training, the 3D random noisy boxes are employed from the 3D ground truth boxes by progressively adding Gaussian noise, and the DDet3D network is trained to reverse the diffusion process. During the inference stage, the DDet3D network is able to iteratively refine the 3D randomly generated and supervised learning-based noisy boxes to predict 3D bounding boxes conditioned on the LiDAR Bird’s Eye View (BEV) features. The advantage of DDet3D is that it allows to decouple training and inference stages, thus enabling the use of a larger number of proposal boxes or sampling steps during inference to improve accuracy. We conduct extensive experiments and analysis on the nuScenes and KITTI datasets. DDet3D achieves competitive performance compared to well-designed 3D object detectors. Our work serves as a strong baseline to explore and employ more efficient diffusion models for 3D perception tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ddet3d</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DDet3D: Embracing 3D Object Detector with Diffusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-Fusion</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dafdetr-480.webp 480w,/assets/img/publication_preview/dafdetr-800.webp 800w,/assets/img/publication_preview/dafdetr-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/dafdetr.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dafdetr.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dafdetr" class="col-sm-8"> <div class="title">DAFDeTr: Deformable Attention Fusion Based 3D Detection Transformer</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>In Robotics, Computer Vision and Intelligent Systems</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-59057-3_19" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/MVAg9ydZuBg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/DAFDeTr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Existing approaches fuse the LiDAR points and image pixels by hard association relying on highly accurate calibration matrices. We propose Deformable Attention Fusion based 3D Detection Transformer (DAFDeTr) to attentively and adaptively fuse the image features to the LiDAR features with soft association using deformable attention mechanism. Specifically, our detection head consists of two decoders for sequential fusion: LiDAR and image decoder powered by deformable cross-attention to link the multi-modal features to the 3D object predictions leveraging a sparse set of object queries. The refined object queries from the LiDAR decoder attentively fuse with the corresponding and required image features establishing a soft association, thereby making our model robust for any camera malfunction. We conduct extensive experiments and analysis on nuScenes and Waymo datasets. Our DAFDeTr-L achieves 63.4 mAP and outperforms well established networks on the nuScenes dataset and obtains competitive performance on the Waymo dataset. Our fusion model DAFDeTr achieves 64.6 mAP on the nuScenes dataset. We also extend our model to the 3D tracking task and our model outperforms state-of-the-art methods on 3D tracking.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dafdetr</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Filipe, Joaquim and R{\"o}ning, Juha}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DAFDeTr: Deformable Attention Fusion Based 3D Detection Transformer}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics, Computer Vision and Intelligent Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature Switzerland}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{293-315}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-031-59057-3}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Pose and Depth</abbr> </div> <div id="10.1117/12.3006235" class="col-sm-8"> <div class="title">Self-supervised monocular pose and depth estimation for wireless capsule endoscopy with transformers</div> <div class="author"> Nahid Nazifi, Helder Araujo, <em>Gopi Krishna Erabati</em>, and Omar Tahri </div> <div class="periodical"> <em>In Medical Imaging 2024: Image-Guided Procedures, Robotic Interventions, and Modeling</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Wireless Capsule Endoscopy (WCE) is an emerging diagnostic technology to examine the Gastrointestinal tract and detect a wide range of diseases and pathologies by capturing images and transferring them remotely. The necessity of having control over the movement of the capsule is crucial to get more accurate detection of the location of the capsule, potential diseased areas, biopsy and drug delivery. However, several challenges are present for WCE, notably the deformable nature of the soft tissues, and texture-less surfaces which are subjected to strong specular reflections. To address these issues and since a reliable real-time 3D pose estimation is critical for controlling active endoscopic capsule robots, this work proposes a data-driven approach to estimate the pose and depth estimation of a wireless capsule endoscope. With recent advances in transformer networks in computer vision tasks, we introduce a Transformer-based architecture to use the self-attention mechanism for specular reflections and deformable topography of the Gastrointestinal tract. This would be a step toward developing a fully autonomous capsule endoscopy for more precise diagnostics and treatments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1117/12.3006235</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nazifi, Nahid and Araujo, Helder and Erabati, Gopi Krishna and Tahri, Omar}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Self-supervised monocular pose and depth estimation for wireless capsule endoscopy with transformers}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12928}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Medical Imaging 2024: Image-Guided Procedures, Robotic Interventions, and Modeling}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Siewerdsen, Jeffrey H. and Rettmann, Maryam E.}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{International Society for Optics and Photonics}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{SPIE}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1292816}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{pose estimation, depth estimation, wireless capsule endoscopy, transformers, deep learning, localization, WCE, visual odometry}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1117/12.3006235}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1117/12.3006235}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-LiDAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Li3DeTr-480.webp 480w,/assets/img/publication_preview/Li3DeTr-800.webp 800w,/assets/img/publication_preview/Li3DeTr-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Li3DeTr.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Li3DeTr.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li3detr" class="col-sm-8"> <div class="title">Li3DeTr: A LiDAR Based 3D Detection Transformer</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Erabati_Li3DeTr_A_LiDAR_Based_3D_Detection_Transformer_WACV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://openaccess.thecvf.com/content/WACV2023/supplemental/Erabati_Li3DeTr_A_LiDAR_WACV_2023_supplemental.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://youtu.be/5pLnLRO_2-U" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/Li3DeTr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Inspired by recent advances in vision transformers for object detection, we propose Li3DeTr, an end-to-end LiDAR based 3D Detection Transformer for autonomous driving, that inputs LiDAR point clouds and regresses 3D bounding boxes. The LiDAR local and global features are encoded using sparse convolution and multi-scale deformable attention respectively. In the decoder head, firstly, in the novel Li3DeTr cross-attention block, we link the LiDAR global features to 3D predictions leveraging the sparse set of object queries learnt from the data. Secondly, the object query interactions are formulated using multi-head self-attention. Finally, the decoder layer is repeated Ldec number of times to refine the object queries. Inspired by DETR, we employ set-to-set loss to train the Li3DeTr network. Without bells and whistles, the Li3DeTr network achieves 61.3% mAP and 67.6% NDS surpassing the state-of-the-art methods with non-maximum suppression (NMS) on the nuScenes dataset and it also achieves competitive performance on the KITTI dataset. We also employ knowledge distillation (KD) using a teacher and student model that slightly improves the performance of our network.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li3detr</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Li3DeTr: A LiDAR Based 3D Detection Transformer}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4250-4259}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-Fusion</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/msf3ddetr_comprs-480.webp 480w,/assets/img/publication_preview/msf3ddetr_comprs-800.webp 800w,/assets/img/publication_preview/msf3ddetr_comprs-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/msf3ddetr_comprs.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="msf3ddetr_comprs.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="msf3ddetr" class="col-sm-8"> <div class="title">MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer for Autonomous Driving</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>In ICPR 2022 workshop on Deep Learning for Visual Detection and Recognition (DLVDR)</em> , Aug 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dlvdr2022.github.io/home/pdf/CameraReady%2015.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/DnES-iQ37v8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/MSF3DDETR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>3D object detection is a significant task for autonomous driving. Recently with the progress of vision transformers, the 2D object detection problem is being treated with the set-to-set loss. Inspired by these approaches on 2D object detection and an approach for multi-view 3D object detection DETR3D, we propose MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer architecture to fuse image and LiDAR features to improve the detection accuracy. Our end-to-end single-stage, anchor-free and NMS-free network takes in multi-view images and LiDAR point clouds and predicts 3D bounding boxes. Firstly, we link the object queries learnt from data to the image and LiDAR features using a novel MSF3DDETR cross-attention block. Secondly, the object queries interacts with each other in multi-head self-attention block. Finally, MSF3DDETR block is repeated for L number of times to refine the object queries. The MSF3DDETR network is trained end-to-end on the nuScenes dataset using Hungarian algorithm based bipartite matching and set-to-set loss inspired by DETR. We present both quantitative and qualitative results which are competitive to the state-of-the-art approaches.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Moving Obj Seg</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mosnetres-480.webp 480w,/assets/img/publication_preview/mosnetres-800.webp 800w,/assets/img/publication_preview/mosnetres-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mosnetres.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mosnetres.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mosnet" class="col-sm-8"> <div class="title">MOSNet: A lightweight Moving Object Segmentation Network for Autonomous Driving</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>In RECPAD 2021 - 27th Portuguese Conference on Pattern Recognition</em> , Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aprp.pt/wp-content/uploads/proceedings_recpad2021.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/S56btzltRTM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>The ability to segment moving objects like cars is a very crucial element of visual perception system of autonomous vehicles for safe manoeuvrability of vehicles. In this paper, we aim to propose a light-weight Moving Object Segmentation Network (MOSNet) which adapts a two-stream architecture to extract appearance and motion features from RGB images and optical flow respectively. The extracted features are fused with the help of a fusion transformer, a Feature Pyramid Network (FPN) head is used to combine feature maps at various scales and further they are bilinearly upsampled to get back to the original dimension of the input which produces per-pixel class. The network is trained and tested on publicly available KITTI MOD dataset. It is shown that the proposed architecture achieves the Intersection over Union (IoU) of 48.89 % for moving objects and also runs at 50 fps on a RTX 2080 Ti GPU using a ShuffleNetV2 backbone.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mosnet</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MOSNet: A lightweight Moving Object Segmentation Network for Autonomous Driving}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{RECPAD 2021 - 27th Portuguese Conference on Pattern Recognition}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7-8}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">2DObjDet</abbr> </div> <div id="objdetcomp" class="col-sm-8"> <div class="title">Object Detection in Traffic Scenarios - A Comparison of Traditional and Deep Learning Approaches</div> <div class="author"> <em>Gopi Krishna Erabati</em>, Nuno Gonçalves, and Helder Araujo </div> <div class="periodical"> <em>In Proceedings of 9th International Conference on Advanced Information Technologies and Applications (ICAITA 2020)</em> , Aug 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aircconline.com/csit/papers/vol10/csit100918.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In the area of computer vision, research on object detection algorithms has grown rapidly as it is the fundamental step for automation, specifically for self-driving vehicles. This work presents a comparison of traditional and deep learning approaches for the task of object detection in traffic scenarios. The handcrafted feature descriptor like Histogram of oriented Gradients (HOG) with a linear Support Vector Machine (SVM) classifier is compared with deep learning approaches like Single Shot Detector (SSD) and You Only Look Once (YOLO), in terms of mean Average Precision (mAP) and processing speed. SSD algorithm is implemented with different backbone architectures like VGG16, MobileNetV2 and ResNeXt50, similarly YOLO algorithm with MobileNetV1 and ResNet50, to compare the performance of the approaches. The training and inference is performed on PASCAL VOC 2007 and 2012 training, and PASCAL VOC 2007 test data respectively. We consider five classes relevant for traffic scenarios, namely, bicycle, bus, car, motorbike and person for the calculation of mAP. Both qualitative and quantitative results are presented for comparison. For the task of object detection, the deep learning approaches outperform the traditional approach both in accuracy and speed. This is achieved at the cost of requiring large amount of data, high computation power and time to train a deep learning approach.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-Fusion</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sl3d-480.webp 480w,/assets/img/publication_preview/sl3d-800.webp 800w,/assets/img/publication_preview/sl3d-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/sl3d.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sl3d.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sl3d" class="col-sm-8"> <div class="title">SL3D - Single Look 3D Object Detection based on RGB-D Images</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>In 2020 Digital Image Computing: Techniques and Applications (DICTA)</em> , Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9363404" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We present SL3D, Single Look 3D object detection approach to detect the 3D objects from the RGB-D image pair. The approach is a proposal free, single-stage 3D object detection method from RGB-D images by leveraging multi-scale feature fusion of RGB and depth feature maps, and multi-layer predictions. The method takes pair of RGB and depth images as an input and outputs predicted 3D bounding boxes. The neural network SL3D, comprises of two modules: multi-scale feature fusion and multi-layer prediction. The multi-scale feature fusion module fuses the multi-scale features from RGB and depth feature maps, which are later used by the multi-layer prediction module for 3D object detection. Each location of prediction layer is attached with a set of predefined 3D prior boxes to account for varying shapes of 3D objects. The output of the network regresses the predicted 3D bounding boxes as an offset to the set of 3D prior boxes and duplicate 3D bounding boxes are removed by applying 3D non-maximum suppression. The network is trained end-to-end on publicly available SUN RGB-D dataset. The SL3D approach with ResNeXt50 achieves 31.77 mAP on SUN RGB-D test dataset with an inference speed of approximately 4 fps, and with MobileNetV2, it achieves approximately 15 fps with a reduction of around 2 mAP. The quantitative results show that the proposed method achieves competitive performance to state-of-the-art methods on SUN RGB-D dataset with near real-time inference speed.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">2DObjDet</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icdsc-480.webp 480w,/assets/img/publication_preview/icdsc-800.webp 800w,/assets/img/publication_preview/icdsc-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icdsc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icdsc.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3349801.3357134" class="col-sm-8"> <div class="title">Dynamic Obstacle Detection in Traffic Environments</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>In Proceedings of the 13th International Conference on Distributed Smart Cameras</em> , Trento, Italy, Nov 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3349801.3357134" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The research on autonomous vehicles has grown increasingly with the advent of neural networks. Dynamic obstacle detection is a fundamental step for self-driving vehicles in traffic environments. This paper presents a comparison of state-of-art object detection techniques like Faster R-CNN, YOLO and SSD with 2D image data. The algorithms for detection in driving, must be reliable, robust and should have a real time performance. The three methods are trained and tested on PASCAL VOC 2007 and 2012 datasets and both qualitative and quantitative results are presented. SSD model can be seen as a tradeoff for speed and small object detection. A novel method for object detection using 3D data (RGB and depth) is proposed. The proposed model incorporates two stage architecture modality for RGB and depth processing and later fused hierarchically. The model will be trained and tested on RGBD dataset in the future.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3349801.3357134</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Obstacle Detection in Traffic Environments}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450371896}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3349801.3357134}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3349801.3357134}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 13th International Conference on Distributed Smart Cameras}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Trento, Italy}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ICDSC 2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Microwave</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pictwt-480.webp 480w,/assets/img/publication_preview/pictwt-800.webp 800w,/assets/img/publication_preview/pictwt-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/pictwt.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pictwt.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="7929140" class="col-sm-8"> <div class="title">Particle-in-cell simulations of CC-TWT for radar transmitters</div> <div class="author"> Latha Christie, and <em>Gopikrishna Erabati</em> </div> <div class="periodical"> <em>In 2016 International Symposium on Antennas and Propagation (APSYM)</em> , Dec 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/7929140" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>TWT is one of the fundamental components in a Radar Transmitter and compared to Helix TWT, Coupled Cavity TWT (CCTWT) gives the highest power over a moderate bandwidth. A complete simulation of the TWT is very useful in avoiding iterations and failures in TWT development cycle. In this paper, the CCTWT designed in the X-band frequency range, has been simulated using the Eigen mode solver and the particle-in-cell solver of the three-dimensional software package, CST Microwave Studio. The slow wave structure along with the couplers is designed initially with the equivalent circuit approach and later optimized using the Eigen mode solver of CST MWS. The initial estimate of the total number of cavities and the number of cavities per section was obtained using large signal analysis which was later optimized using Particle-in-cell Solver of CST MWS. The Simulation predicted an output power of around 1 kW and 27 cavities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">7929140</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Christie, Latha and Erabati, Gopikrishna}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2016 International Symposium on Antennas and Propagation (APSYM)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Particle-in-cell simulations of CC-TWT for radar transmitters}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/APSYM.2016.7929140}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Microwave</abbr> </div> <div id="iet:/content/conferences/10.1049/cp.2016.1523" class="col-sm-8"> <div class="title">Homogeneous and inhomogeneous coupling structures for coupled cavity TWTS</div> <div class="author"> <em>Gopikrishna Erabati</em> </div> <div class="periodical"> <em>IET Conference Proceedings</em>, Jan 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Analysis and optimization of performance of coupling structures of Coupled Cavity Travelling Wave Tube (CC-TWT) that are used to feed and extract RF power into and from the TWT is presented. The design of coupling structures includes the design of hybrid cavity and the design of stepped impedance transformer for transforming the impedance of waveguide to that of the cavity. The stepped impedance transformer can be homogeneous or inhomogeneous based on the design of magnets near the coupling end of the tube. In this paper the design of both the coupling structures. In this paper, the complete design of the coupling structure for coupled cavity TWT using the hybrid cavities and the stepped impedance transformer, both the homogeneous and the inhomogeneous type is presented. The analytical design is compared with that of 3-Dimensional (3D) electromagnetic simulation software, CST Microwave Studio (MWS) and the results presented.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">iet:/content/conferences/10.1049/cp.2016.1523</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopikrishna}</span><span class="p">,</span>
  <span class="na">affiliation</span> <span class="p">=</span> <span class="s">{Microwave Tube R&amp;amp;D Centre, Bangalore}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{3-Dimensional electromagnetic simulation software;inhomogeneous coupling structures;homogeneous coupling structures;Coupled Cavity Travelling Wave Tube;stepped impedance transformer;coupled cavity TWTS;coupling structure;CST Microwave Studio;hybrid cavity;}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{English}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Homogeneous and inhomogeneous coupling structures for coupled cavity TWTS}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IET Conference Proceedings}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{-(1)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Institution of Engineering and Technology}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://digital-library.theiet.org/content/conferences/10.1049/cp.2016.1523}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Microwave</abbr> </div> <div id="mondalanalysis" class="col-sm-8"> <div class="title">Analysis of H-Plane Discontinuity in a Rectangular Waveguide using Mode Matching Technique</div> <div class="author"> Latha Christie, Payel Mondal, Sritama Dutta, and <em>Gopikrishna Erabati</em> </div> <div class="periodical"> <em>INROADS- An International Journal of Jaipur National University</em>, Jan 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Accurate determination of S-parameters using Mode Matching Technique of H-plane discontinuity is presented. The generalized scattering matrix is obtained from the respective field equations. The H-plane discontinuity operating in X-Band has been considered for a case study. The results that have been obtained using Mode Matching Technique are compared with equivalent circuit approach and 3-D EM simulation software CST MWS and HFSS which are based on Finite Integration Technique and Finite Element Method respectively, based on accuracy and simulation time. The error between Mode Matching Technique and CST MWS is found to be less than 1% with the lowest simulation time.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mondalanalysis</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Christie, Latha and Mondal, Payel and Dutta, Sritama and Erabati, Gopikrishna}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Analysis of H-Plane Discontinuity in a Rectangular Waveguide using Mode Matching Technique}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{INROADS- An International Journal of Jaipur National University}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">vol</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{1s}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{367-371}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.5958/2277-4912.2016.00069.2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Microwave</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ddisk-480.webp 480w,/assets/img/publication_preview/ddisk-800.webp 800w,/assets/img/publication_preview/ddisk-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ddisk.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ddisk.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="7433812" class="col-sm-8"> <div class="title">Analysis of Propagation Characteristics of Circular Waveguide Loaded with Dielectric Disks Using Coupled Integral Equation Technique</div> <div class="author"> Latha Christie, <em>Gopikrishna Erabati</em>, and Mita Jana </div> <div class="periodical"> <em>In 2015 Fifth International Conference on Advances in Computing and Communications (ICACC)</em> , Sep 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/7433812" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Coupled Integral Equation Technique (CIET) is presented for the study of propagation characteristics of circular waveguide periodically loaded with dielectric disks operating in TM01d mode. CIET is a combination of Mode Matching Technique (MMT) and Method of Moments. The results are presented for three materials having different dielectric constants and compared with 3D simulation tool CST Studio in terms of simulation time and accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">7433812</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Christie, Latha and Erabati, Gopikrishna and Jana, Mita}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2015 Fifth International Conference on Advances in Computing and Communications (ICACC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Analysis of Propagation Characteristics of Circular Waveguide Loaded with Dielectric Disks Using Coupled Integral Equation Technique}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{231-234}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICACC.2015.115}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Microwave</abbr> </div> <div id="7433813" class="col-sm-8"> <div class="title">Transverse Focusing Structure for TWTs</div> <div class="author"> Mita Jana, Latha Christie, and <em>Gopikrishna Erabati</em> </div> <div class="periodical"> <em>In 11th International Conference on Microwaves, Antenna, Propagation and Remote Sensing (ICMARS 2015)</em> , Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ris.utwente.nl/ws/portalfiles/portal/48827552/ICMARS2015Proceedings.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">7433813</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jana, Mita and Christie, Latha and Erabati, Gopikrishna}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{11th International Conference on Microwaves, Antenna, Propagation and Remote Sensing (ICMARS 2015)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transverse Focusing Structure for TWTs}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{52-53}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Gopi Krishna Erabati. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?fa0110e8b42cec56ce96d912fd4bde74"></script> <script>addBackToTop();</script> </body> </html>