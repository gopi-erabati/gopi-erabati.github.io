<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gopi Krishna Erabati </title> <meta name="author" content="Gopi Krishna Erabati"> <meta name="description" content="A PhD candidate at Institute of Systems and Robotics, University of Coimbra, Portugal, focusing on Scene Understanding for Autonomous Driving. "> <meta name="keywords" content="Computer Vision, Autonomous Driving, Deel Learning, Scene Understanding"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/6.jpg?b2748a8c220f6715934bfddac3f6df51"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gopi-erabati.github.io/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%67%6F%70%69.%65%72%61%62%61%74%69@%69%73%72.%75%63.%70%74" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=9bKDuggAAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Gopi-Krishna-Erabati/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/gopi-erabati" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/gopierabati" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/gopi_chirps" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://justatenderfoot.blogspot.com/" title="Blogger" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-blogger-b"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/experience/">Experience </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">Awards </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_Gopi.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Gopi Krishna</span> Erabati </h1> <p class="desc">Marie Curie Ph.D. Candidate at <a href="https://isr.uc.pt/" rel="external nofollow noopener" target="_blank">Institute of Systems and Robotics, University of Coimbra, Portugal</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pic_gopi-480.webp 480w,/assets/img/pic_gopi-800.webp 800w,/assets/img/pic_gopi-1400.webp 1400w," sizes="(min-width: 1000px) 291.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/pic_gopi.jpg?01fdb9721158782c44e54078fa7a4d2f" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="pic_gopi.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p>I am currently a Ph.D. candidate working on <strong>Scene Understanding for Autonomous Driving</strong> at the <a href="https://isr.uc.pt/" rel="external nofollow noopener" target="_blank">Institute of Systems and Robotics, University of Coimbra, Portugal</a>. I am fortunate to be advised by <a href="https://orcid.org/0000-0002-9544-424X" rel="external nofollow noopener" target="_blank">Prof. Helder Araujo</a>. My primary research interests are focused on computer vision and machine learning, specifically tasks related to scene understanding.</p> <p>During my Ph.D. thesis, I worked on fundamental scene understanding tasks such as 3D object detection using LiDAR and multi-modal data fusion, 3D semantic segmentation, and panoptic driving perception.</p> <p>Prior to joining UC, I obtained a M.Sc. degree in Computer Vision in the <a href="https://www.vibot.org/" rel="external nofollow noopener" target="_blank">Erasmus VIBOT (VIsion and RoBOTics)</a> program at the <a href="https://en.u-bourgogne.fr/" rel="external nofollow noopener" target="_blank">Université de Dijon, France</a>. I had done my master thesis advised by <a href="https://homepages.laas.fr/lerasle/" rel="external nofollow noopener" target="_blank">Prof. Frédéric Lerasle</a> and my summer internship advised by <a href="https://homepages.laas.fr/simon/HomePage/Home.html" rel="external nofollow noopener" target="_blank">Dr. Simon Lacroix</a> at <a href="https://www.laas.fr/en/" rel="external nofollow noopener" target="_blank">LAAS-CNRS, France</a>. I had obtained a B.Tech. degree in Electronics and Instrumentation Engineering at <a href="https://kitsw.ac.in/" rel="external nofollow noopener" target="_blank">Kakatiya University, India</a>. I received a <em>Gold Medal from the Hon’ble Governor of Telangana, India, for excellence in academics</em> during my bachleor’s degree at Kakatiya University, India.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jan 31, 2025</th> <td> Our SCAM-P paper received acceptance for IEEE ICRA 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 11, 2024</th> <td> Our <a href="https://doi.org/10.1016/j.cviu.2024.104231" rel="external nofollow noopener" target="_blank">RetSeg3D</a> paper got accepted in <em>Computer Vision and Image Understanding (CVIU)</em> journal. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 09, 2024</th> <td> Our <em><a href="https://doi.org/10.1007/s10489-024-06045-1" rel="external nofollow noopener" target="_blank">DDet3D</a></em> paper got accepted in <em>Applied Intelligence</em> journal. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 16, 2024</th> <td> Our <a href="https://doi.org/10.1109/TIV.2024.3417260" rel="external nofollow noopener" target="_blank">RetFormer</a> paper got accepted in <em>IEEE Transactions on Intelligent Vehicles (IEEE T-IV)</em> journal. </td> </tr> <tr> <th scope="row" style="width: 20%">May 05, 2024</th> <td> Our <a href="https://doi.org/10.1016/j.neucom.2024.127814" rel="external nofollow noopener" target="_blank">SRFDet3D</a> paper got accepted in <em>Neurocomputing</em> journal. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <p>For full list click <a href="https://gopi-erabati.github.io/publications/">here</a>!</p> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Panoptic</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/scamp-480.webp 480w,/assets/img/publication_preview/scamp-800.webp 800w,/assets/img/publication_preview/scamp-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/scamp.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="scamp.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="scamp" class="col-sm-8"> <div class="title">SCAM-P: Spatial Channel Attention Module for Panoptic Driving Perception</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>In 2025 IEEE International Conference on Robotics and Automation (ICRA)</em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/l-3_dEx6FPw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>A high-precision, high-efficiency, and lightweight panoptic driving perception system is an essential part of autonomous driving for optimal maneuver planning of the autonomous vehicle. We propose a simple, lightweight, and efficient SCAM-P multi-task learning network that accomplishes three crucial tasks simultaneously for panoptic driving: vehicle detection, drivable area segmentation, and lane segmentation. To increase the representation power of the shared backbone of our multi-task network, we designed a novel SCAM module with spatially localized channel attention and channel localized spatial attention blocks. SCAM is a lightweight module that can be plugged into any CNN architecture to enhance the semantic features with negligible computational overhead. We integrate our SCAM module and design the SCAM-P network, which has a shared backbone for feature extraction and three independent heads to handle three tasks at the same time. We also designed a nano variant of our SCAM-P network to make it deployment-friendly on edge devices. Our SCAM-P network obtains competitive results on the BDD100K dataset with 81.1 % mAP50 for object detection, 91.6 % mIoU for drivable area segmentation, and 28.8 % IoU for lane segmentation. Our model is robust in various adverse weather conditions, such as rainy, snowy, and at night. Our SCAM-P network not only achieves improved performance but also runs efficiently in real-time at 230.5 FPS on the RTX 4090 GPU and 112.1 FPS on the Jetson Orin edge device.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">scamp</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SCAM-P: Spatial Channel Attention Module for Panoptic Driving Perception}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-LiDAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/retformer-480.webp 480w,/assets/img/publication_preview/retformer-800.webp 800w,/assets/img/publication_preview/retformer-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/retformer.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="retformer.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="retformer" class="col-sm-8"> <div class="title">RetFormer: Embracing Point Cloud Transformer with Retentive Network</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>IEEE Transactions on Intelligent Vehicles</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10568420" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/mxJ7_OsqMiQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/RetFormer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Point Cloud Transformers (PCTs) have gained lot of attention not only on the indoor data but also on the large-scale outdoor 3D point clouds, such as in autonomous driving. However, the vanilla self-attention mechanism in PCTs does not include any explicit prior spatial information about the quantized voxels (or pillars). Recently, Retentive Network has gained attention in the natural language processing (NLP) domain due to its efficient modelling capability and remarkable performance, leveraged by the introduction of explicit decay mechanism which incorporates the distance related spatial prior knowledge into the model. As the NLP tasks are causal and one-dimensional in nature, the explicit decay is designed to be unidirectional and one-dimensional. However, the pillars in the Bird’s Eye View (BEV) space are two-dimensional without causal properties. In this work, we propose RetFormer model by introducing bidirectional and two-dimensional decay mechanism for pillars in PCT and design the novel Multi-Scale Retentive Self-Attention (MSReSA) module. The introduction of explicit bidirectional and two-dimensional decay incorporates the 2D spatial distance related prior information of pillars into the PCT which significantly improves the modelling capacity of RetFormer. We evaluate our method on large-scale Waymo and KITTI datasets. RetFormer not only achieves significant performance gain over of 2.3 mAP and 3.2 mAP over PCT-based SST and FlatFormer respectively, and 6.4 mAP over sparse convolutional-based CenterPoint for small object pedestrian category on Waymo Open Dataset, but also is efficient with 3.2x speedup over SST and runs in real-time at  69 FPS on a RTX 4090 GPU.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">retformer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RetFormer: Embracing Point Cloud Transformer with Retentive Network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Intelligent Vehicles}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-Fusion</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/srfdet3d-480.webp 480w,/assets/img/publication_preview/srfdet3d-800.webp 800w,/assets/img/publication_preview/srfdet3d-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/srfdet3d.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="srfdet3d.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="srfdet3d" class="col-sm-8"> <div class="title">SRFDet3D: Sparse Region Fusion based 3D Object Detection</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>Neurocomputing</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pdf.sciencedirectassets.com/271597/1-s2.0-S0925231224X00235/1-s2.0-S092523122400585X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCTNNlwz75MW1wq3pgkpoMyugISNB3I9%2BsS14Oao9kYRQIhAJ0o1rXFZn0Jbtif7E5x8QgSqpe0BxxKi957z7umaMCtKrMFCFsQBRoMMDU5MDAzNTQ2ODY1Igw6nmg5KcyFkEAhvA0qkAW3iVZqKqBQv0NjdVN2vSej1k0gNkQj9emw3Z6J2x9pxdMQKzuM89xbplMpslXNoDjmO9H9AJ0QZWMTbefdkEUwSskVpubTPgOT%2FY5M17XEc8xxDC9W1%2FlygwK5oZqtpSbY8zuhsNGfRnAZdgXU2ouAxfbL726M2j%2FYXmu9gVXyi6x074x5E8TxNiwptxG%2BWe02SJM0F3%2BDqu%2BY6k7UyEEJsHZ118Zd1jMFJCGpBFOGCkOHH%2F%2BHHSd4Wr8vgqlqLlx07VMXX37EsoQgebzZYfRDmRsgvO6O38fiKh3qo1f95MMIbtRIasA6TRJESl1LGcVFwHhZ6tGHoRqQW0O1W%2Bs8v2vFxnYUKX41kE%2FiTYvVnSGu3qJ08UNZtq8BuhCwTQYPLe%2FZcyFK6coIIcTdtBBCbRruz4zXEiuuN6nq2QZGeeU7EHww8e69qiBRlEXEqDK8QiG8fJb7TMVTe2%2Bu%2BWZz3dixQz87A1ZgNOnX%2BtR5YHlLoAOm6Ley7H49ixOf0nuagO7dMKz2aHFzH1u%2BUv%2BHMNj2RfL0Jfsvog6m%2B2In5Va%2BT5p4mgBw2Rt%2FL9kUoJnRNEQF%2Fgl9jaszEt5sByyhnkbDiTDkCvBWkgO3l1Aq2v9oXUiM2%2Brs2wMTB41cReNOgMfV2KQV2rY4fpcQAX1VwPlsKxx7i5BJCHNNrujTy7Kj4rpMeBXq8GrAL3G06PfJtZNfHm9g0VfVzWVFeV94X5tC4PMnDjDxVdcV2K85wCOJOTSWgPUqaws3STY0eNNp7XSl7Ee9yTtdN7H54Wy5P353zTwxQj1mjiOkTEXwP4CI9UajOWtHuh7hRrHS61oKmTlwkBb%2FHoxCYz6rEUC8I3X0cbCksejSD12RSlJYWjCribeyBjqwAWGcW66D%2BeTE8iElxKxNmeM3i9Vb4eI02Ujiya4AzOuDpjTm%2Fo2NgKq0QymubwCtYgyWA26ixNg3pmD%2F%2FmfnkXarWB%2BQTRRDDwpdYCRd7rVsezSV4oiysq8zogPdzxg5IvQpsV2PJd2KeBlN9zRJe5Tm7offqCnvztmcfe2Ghd4RIgC5DjLTlnKFZy0aclNtszWOlQ%2BEb0TBBzQf56y3JjUpb%2Be6zVsd8s9pBLhV98CL&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240522T113701Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=299&amp;X-Amz-Credential=ASIAQ3PHCVTY3XX57AWT%2F20240522%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=8fca4171ab6c16c40bea80c1525e495bd397297716c7e005d345454d071635d6&amp;hash=1004c7480a8be94f156089de54e8a9841ce22e1f136f5900e3db7e5b98460e62&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S092523122400585X&amp;tid=spdf-df21eaf1-98b7-42e3-8897-a440afea22b1&amp;sid=3c33f25858a0c2495e6b318-a4f5072dd0a8gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=16125d5759065c555505&amp;rr=887c830a5abe692c&amp;cc=pt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/2yNej0T4w0Q" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/SRFDet3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Unlike the earlier 3D object detection approaches that formulate hand-crafted dense (in thousands) object proposals by leveraging anchors on dense feature maps, we formulate np (in hundreds) number of learnable sparse object proposals to predict 3D bounding box parameters. The sparse proposals in our approach are not only learnt during training but also are input-dependent, so they represent better object candidates during inference. Leveraging the sparse proposals, we fuse only the sparse regions of multi-modal features and we propose Sparse Region Fusion based 3D object Detection (SRFDet3D) network with mainly three components: an encoder for feature extraction, a region proposal generation module for sparse input-dependent proposals and a decoder for multi-modal feature fusion and iterative refinement of object proposals. Additionally for optimal training, we formulate our sparse detector with many-to-one label assignment based on Optimal Transport Algorithm (OTA). We conduct extensive experiments and analysis on publicly available large-scale autonomous driving datasets: nuScenes, KITTI, and Waymo. Our LiDAR-only SRFDet3D-L network achieves 63.1 mAP and outperforms the state-of-the-art networks on the nuScenes dataset, surpassing the dense detectors on KITTI and Waymo datasets. Our LiDAR-Camera model SRFDet3D achieves 64.7 mAP with improvements over existing fusion methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">srfdet3d</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SRFDet3D: Sparse Region Fusion based 3D Object Detection}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neurocomputing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{593}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{127814}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0925-2312}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.neucom.2024.127814}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S092523122400585X}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{3D object detection, Fusion, Camera, LiDAR, Autonomous driving, Computer vision}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DSemSeg</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/retseg3d-480.webp 480w,/assets/img/publication_preview/retseg3d-800.webp 800w,/assets/img/publication_preview/retseg3d-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/retseg3d.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="retseg3d.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="retseg3d" class="col-sm-8"> <div class="title">RetSeg3D: Retention-based 3D semantic segmentation for autonomous driving</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>Computer Vision and Image Understanding</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1016/j.cviu.2024.104231" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/aBN-v0A_WfU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/RetSeg3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>LiDAR semantic segmentation is one of the crucial tasks for scene understanding in autonomous driving. Recent trends suggest that voxel- or fusion-based methods obtain improved performance. However, the fusion-based methods are computationally expensive. On the other hand, the voxel-based methods uniformly employ local operators (e.g., 3D SparseConv) without considering the varying-density property of LiDAR point clouds, which result in inferior performance, specifically on far away sparse points due to limited receptive field. To tackle this issue, we propose novel retention block to capture long-range dependencies, maintain the receptive field of far away sparse points and design RetSeg3D, a retention-based 3D semantic segmentation model for autonomous driving. Instead of vanilla attention mechanism to model long-range dependencies, inspired by RetNet, we design cubic window multi-scale retentive self-attention (CW-MSRetSA) module with bidirectional and 3D explicit decay mechanism to introduce 3D spatial distance related prior information into the model to improve not only the receptive field but also the model capacity. Our novel retention block maintains the receptive field which significantly improve the performance of far away sparse points. We conduct extensive experiments and analysis on three large-scale datasets: SemanticKITTI, nuScenes and Waymo. Our method not only outperforms existing methods on far away sparse points but also on close and medium distance points and efficiently runs in real time at 52.1 FPS on a RTX 4090 GPU.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">retseg3d</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RetSeg3D: Retention-based 3D semantic segmentation for autonomous driving}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Image Understanding}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{104231}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1077-3142}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.cviu.2024.104231}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S1077314224003126}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{3D semantic segmentation, Retention, Autonomous driving, LiDAR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-LiDAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Li3DeTr-480.webp 480w,/assets/img/publication_preview/Li3DeTr-800.webp 800w,/assets/img/publication_preview/Li3DeTr-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/Li3DeTr.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Li3DeTr.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="li3detr" class="col-sm-8"> <div class="title">Li3DeTr: A LiDAR Based 3D Detection Transformer</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Erabati_Li3DeTr_A_LiDAR_Based_3D_Detection_Transformer_WACV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://openaccess.thecvf.com/content/WACV2023/supplemental/Erabati_Li3DeTr_A_LiDAR_WACV_2023_supplemental.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://youtu.be/5pLnLRO_2-U" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/Li3DeTr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Inspired by recent advances in vision transformers for object detection, we propose Li3DeTr, an end-to-end LiDAR based 3D Detection Transformer for autonomous driving, that inputs LiDAR point clouds and regresses 3D bounding boxes. The LiDAR local and global features are encoded using sparse convolution and multi-scale deformable attention respectively. In the decoder head, firstly, in the novel Li3DeTr cross-attention block, we link the LiDAR global features to 3D predictions leveraging the sparse set of object queries learnt from the data. Secondly, the object query interactions are formulated using multi-head self-attention. Finally, the decoder layer is repeated Ldec number of times to refine the object queries. Inspired by DETR, we employ set-to-set loss to train the Li3DeTr network. Without bells and whistles, the Li3DeTr network achieves 61.3% mAP and 67.6% NDS surpassing the state-of-the-art methods with non-maximum suppression (NMS) on the nuScenes dataset and it also achieves competitive performance on the KITTI dataset. We also employ knowledge distillation (KD) using a teacher and student model that slightly improves the performance of our network.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li3detr</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Li3DeTr: A LiDAR Based 3D Detection Transformer}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4250-4259}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-LiDAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/delivotr-480.webp 480w,/assets/img/publication_preview/delivotr-800.webp 800w,/assets/img/publication_preview/delivotr-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/delivotr.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="delivotr.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="delivotr" class="col-sm-8"> <div class="title">DeLiVoTr: Deep and light-weight voxel transformer for 3D object detection</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>Intelligent Systems with Applications</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pdf.sciencedirectassets.com/779210/1-s2.0-S2667305324X00022/1-s2.0-S2667305324000371/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDC4paLsuxn5gKaiguKnMIN8IiGWd3FfoKIgFW59uORxgIhAJdfOmTzSlGX3SDaeFwBNTURUTIZaksRFVt%2BtaLYOR19KrMFCFwQBRoMMDU5MDAzNTQ2ODY1IgxoHV7td%2Fpo%2B0F38AYqkAXdtX3j8RTlaZ%2B15j%2ByqoMLNtLkXGV%2BUKAMggrNdZFfi4%2BiPRE5H%2FO8tseYT2w60P%2BtPaisTvKa3irfdMCPrOPWOqDRCy2Y8LZMxckNs9VeHHmsyKV%2Bz86N%2BCHwq3w%2FYiVVMtjNia5IIzyQDVvpEepAmpS3kOCHepsNsNGBGKUM2bnLf5RTBNc%2BPIxCOp4L97d2M2sG1s5zxq59WBOX1rTiWAP40X9dwNXMLnGHPmdSbPfJu0Z%2FGk6eqJ5rB0s5c6JaZY6r8r2R0l1MfGurywU%2FeUD7hMQ4UK8fnTcm1rfBQ%2BbZ8owFO8bVojVj81D4OJtZIp4qnxNrrAfT4bVkWyiU7rOrj%2Fs1jtrogLutxJdyztZWORIq3UUXOV3PKlZl7o1VYWD7%2FJvtWLbPr9MXyhV4llqP2VGSSz3HvFwnhWvzs9kMrOkUh4tqRfz8iA89kXTWwYrK3LFLij9X9RySub11f%2FgPanG6gWGdI68XV54uqrWINkbhuawqstWEYK%2FYSEhQS5x9t7MlLAOoTAIOvUfi7JT6a6JmDvoHQ6x3hSrABvMH9uED5JCeI3jGdwU1IYpr61HaaSkaw%2FJ6Ly3gky8tnsY7wSBFRQ5mTcru0CbaNWcmZXuuOzGRsYAkelcLdq%2FLECnPtdeuh5603YkRteiCwJFGkYNyY3PoGKRjj0Ypg%2F9C6%2FZtuGSs8DEbR6St8bTkGvo7kJ8LvwY6zmPpCBCtPfNvS9hfpNsUu6u0IDfoMebNRRXK57vayu%2FKuvXrhnwl1yp41mus8mEwsQHPPoTC40J1oFBFFvDQcuAPANvEPKMKrjzQXNoQmZrRnbEjfoXmxGFiW%2FxkGGPpMNoz4sjYflHd7dqgwg86DzDJDHVeQTDglbeyBjqwASZCLgA06JQLcFqlK5%2F4uRHG6ov0Gd%2BcYInT94m39H6tbwlQq5pe2n93jAfBWiDMg7y8wmDl%2BCkSA7xuJ%2BYErOGr2nXp3fZpL6nh%2F9aGBkq1UAJwkjdL0F%2B5Mnug1WHezdXPkz0928K3ePgEkEpFd0iAN6rXBBdhidB2%2BWzuymccNK1C6GNdOw%2BZPxXkJ%2B3%2FBDxE805zFvILW0tuojPHkCwep3bYNVOV8%2BNJIguuVDvg&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240522T113500Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYQIIHXXFU%2F20240522%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=1a25a67ed6846623b8ebeb4f2ca0e10a1909c1f60d3f6d95d17b28039248eb38&amp;hash=b8529fc7651712323c76ea6ed60ae39394386f272b03e534d696b598847d5c03&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S2667305324000371&amp;tid=spdf-51c0fe81-e383-4769-925d-bcfef88d7bd2&amp;sid=3c33f25858a0c2495e6b318-a4f5072dd0a8gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=16125d5759065c565451&amp;rr=887c80156c2f692c&amp;cc=pt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/jnYDfZbVsjw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/DeLiVoTr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The image-based backbone (feature extraction) networks downsample the feature maps not only to increase the receptive field but also to efficiently detect objects of various scales. The existing feature extraction networks in LiDAR-based 3D object detection tasks follow the feature map downsampling similar to image-based feature extraction networks to increase the receptive field. But, such downsampling of LiDAR feature maps in large-scale autonomous driving scenarios hinder the detection of small size objects, such as pedestrians. To solve this issue we design an architecture that not only maintains the same scale of the feature maps but also the receptive field in the feature extraction network to aid for efficient detection of small size objects. We resort to attention mechanism to build sufficient receptive field and we propose a Deep and Light-weight Voxel Transformer (DeLiVoTr) network with voxel intra- and inter-region transformer modules to extract voxel local and global features respectively. We introduce DeLiVoTr block that uses transformations with expand and reduce strategy to vary the width and depth of the network efficiently. This facilitates to learn wider and deeper voxel representations and enables to use not only smaller dimension for attention mechanism but also a light-weight feed-forward network, facilitating the reduction of parameters and operations. In addition to model scaling, we employ layer-level scaling of DeLiVoTr encoder layers for efficient parameter allocation in each encoder layer instead of fixed number of parameters as in existing approaches. Leveraging layer-level depth and width scaling we formulate three variants of DeLiVoTr network. We conduct extensive experiments and analysis on large-scale Waymo and KITTI datasets. Our network surpasses state-of-the-art methods for detection of small objects (pedestrians) with an inference speed of 20.5 FPS.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">delivotr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DeLiVoTr: Deep and light-weight voxel transformer for 3D object detection}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Intelligent Systems with Applications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{200361}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2667-3053}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.iswa.2024.200361}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S2667305324000371}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{3D object detection, Transformer, Voxel, LiDAR, Autonomous driving, Computer vision}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-LiDAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ddet3d-480.webp 480w,/assets/img/publication_preview/ddet3d-800.webp 800w,/assets/img/publication_preview/ddet3d-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ddet3d.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ddet3d.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ddet3d" class="col-sm-8"> <div class="title">DDet3D: Embracing 3D Object Detector with Diffusion</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>Applied Intelligence</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/3XQd5G-wGNM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/DDet3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Existing approaches rely on heuristic or learnable object proposals (which are required to be optimised during training) for 3D object detection. In our approach, we replace the hand-crafted or learnable object proposals with randomly generated object proposals by formulating a new paradigm to employ a diffusion model to detect 3D objects from a set of randomly generated and supervised learning-based object proposals in an autonomous driving application. We propose DDet3D, a diffusion-based 3D object detection framework that formulates 3D object detection as a generative task over the 3D bounding box coordinates in 3D space. To our knowledge, this work is the first to formulate the 3D object detection with denoising diffusion model and to establish that 3D randomly generated and supervised learning-based proposals (different from empirical anchors or learnt queries) are also potential object candidates for 3D object detection. During training, the 3D random noisy boxes are employed from the 3D ground truth boxes by progressively adding Gaussian noise, and the DDet3D network is trained to reverse the diffusion process. During the inference stage, the DDet3D network is able to iteratively refine the 3D randomly generated and supervised learning-based noisy boxes to predict 3D bounding boxes conditioned on the LiDAR Bird’s Eye View (BEV) features. The advantage of DDet3D is that it allows to decouple training and inference stages, thus enabling the use of a larger number of proposal boxes or sampling steps during inference to improve accuracy. We conduct extensive experiments and analysis on the nuScenes and KITTI datasets. DDet3D achieves competitive performance compared to well-designed 3D object detectors. Our work serves as a strong baseline to explore and employ more efficient diffusion models for 3D perception tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ddet3d</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DDet3D: Embracing 3D Object Detector with Diffusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-Fusion</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dafdetr-480.webp 480w,/assets/img/publication_preview/dafdetr-800.webp 800w,/assets/img/publication_preview/dafdetr-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/dafdetr.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dafdetr.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="dafdetr" class="col-sm-8"> <div class="title">DAFDeTr: Deformable Attention Fusion Based 3D Detection Transformer</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>In Robotics, Computer Vision and Intelligent Systems</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-59057-3_19" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/MVAg9ydZuBg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/DAFDeTr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Existing approaches fuse the LiDAR points and image pixels by hard association relying on highly accurate calibration matrices. We propose Deformable Attention Fusion based 3D Detection Transformer (DAFDeTr) to attentively and adaptively fuse the image features to the LiDAR features with soft association using deformable attention mechanism. Specifically, our detection head consists of two decoders for sequential fusion: LiDAR and image decoder powered by deformable cross-attention to link the multi-modal features to the 3D object predictions leveraging a sparse set of object queries. The refined object queries from the LiDAR decoder attentively fuse with the corresponding and required image features establishing a soft association, thereby making our model robust for any camera malfunction. We conduct extensive experiments and analysis on nuScenes and Waymo datasets. Our DAFDeTr-L achieves 63.4 mAP and outperforms well established networks on the nuScenes dataset and obtains competitive performance on the Waymo dataset. Our fusion model DAFDeTr achieves 64.6 mAP on the nuScenes dataset. We also extend our model to the 3D tracking task and our model outperforms state-of-the-art methods on 3D tracking.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dafdetr</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erabati, Gopi Krishna and Araujo, Helder}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Filipe, Joaquim and R{\"o}ning, Juha}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DAFDeTr: Deformable Attention Fusion Based 3D Detection Transformer}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics, Computer Vision and Intelligent Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature Switzerland}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{293-315}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-031-59057-3}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DObjDet-Fusion</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/msf3ddetr_comprs-480.webp 480w,/assets/img/publication_preview/msf3ddetr_comprs-800.webp 800w,/assets/img/publication_preview/msf3ddetr_comprs-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/msf3ddetr_comprs.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="msf3ddetr_comprs.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="msf3ddetr" class="col-sm-8"> <div class="title">MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer for Autonomous Driving</div> <div class="author"> <em>Gopi Krishna Erabati</em>, and Helder Araujo </div> <div class="periodical"> <em>In ICPR 2022 workshop on Deep Learning for Visual Detection and Recognition (DLVDR)</em> , Aug 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dlvdr2022.github.io/home/pdf/CameraReady%2015.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/DnES-iQ37v8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/gopi-erabati/MSF3DDETR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>3D object detection is a significant task for autonomous driving. Recently with the progress of vision transformers, the 2D object detection problem is being treated with the set-to-set loss. Inspired by these approaches on 2D object detection and an approach for multi-view 3D object detection DETR3D, we propose MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer architecture to fuse image and LiDAR features to improve the detection accuracy. Our end-to-end single-stage, anchor-free and NMS-free network takes in multi-view images and LiDAR point clouds and predicts 3D bounding boxes. Firstly, we link the object queries learnt from data to the image and LiDAR features using a novel MSF3DDETR cross-attention block. Secondly, the object queries interacts with each other in multi-head self-attention block. Finally, MSF3DDETR block is repeated for L number of times to refine the object queries. The MSF3DDETR network is trained end-to-end on the nuScenes dataset using Hungarian algorithm based bipartite matching and set-to-set loss inspired by DETR. We present both quantitative and qualitative results which are competitive to the state-of-the-art approaches.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%67%6F%70%69.%65%72%61%62%61%74%69@%69%73%72.%75%63.%70%74" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=9bKDuggAAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Gopi-Krishna-Erabati/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/gopi-erabati" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/gopierabati" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/gopi_chirps" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://justatenderfoot.blogspot.com/" title="Blogger" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-blogger-b"></i></a> </div> <div class="contact-note">Email is the best way to reach me. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Gopi Krishna Erabati. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: February 01, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?fa0110e8b42cec56ce96d912fd4bde74"></script> <script>addBackToTop();</script> </body> </html>