<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Projects | Gopi Krishna Erabati </title> <meta name="author" content="Gopi Krishna Erabati"> <meta name="description" content="A PhD candidate at Institute of Systems and Robotics, University of Coimbra, Portugal, focusing on Scene Understanding for Autonomous Driving. "> <meta name="keywords" content="Computer Vision, Autonomous Driving, Deel Learning, Scene Understanding"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/6.jpg?b2748a8c220f6715934bfddac3f6df51"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gopi-erabati.github.io/projects/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Gopi Krishna</span> Erabati </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/experience/">Experience </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">Awards </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_Gopi.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Projects</h1> <p class="post-description"></p> </header> <article> <h3 id="learning-to-perceive-scene-understanding-for-autonomous-driving">Learning to Perceive: Scene Understanding for Autonomous Driving</h3> <p>PhD Thesis at Institute of Systems and Robotics, University of Coimbra, Portugal</p> <p>Scene understanding is the most fundamental module in the perception system of an autonomous vehicle, as it provides a contextual understanding of the scene, which is required for safe planning and control of the autonomous vehicle. In this thesis, we proposed novel algorithms for scene understanding tasks such as 3D object detection based on LiDAR and multi-modal data, 3D semantic segmentation, and a panoptic driving perception system for autonomous driving.</p> <p>Publications during PhD research work can be found <a href="https://gopi-erabati.github.io/publications/">here</a></p> <p style="text-align: center;">Thesis Overview</p> <p style="text-align:center;"><img src="../assets/img/PhDThesis/thesis-outline.png" width="700" height="350"></p> <table> <thead> <tr> <th style="text-align: center">3D Object Detection (LiDAR)</th> <th style="text-align: center">3D Semantic Segmentation (LiDAR)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><a href="https://youtu.be/cbnHHVaWsuc" rel="external nofollow noopener" target="_blank"><img src="../assets/img/PhDThesis/retformer_ss.png" width="500" height="250" class="center"></a></td> <td style="text-align: center"><a href="https://youtu.be/Yr6Wu0nHeeE" rel="external nofollow noopener" target="_blank"><img src="../assets/img/PhDThesis/retseg3d_ss.png" width="500" height="250" class="center"></a></td> </tr> </tbody> </table> <table> <thead> <tr> <th style="text-align: center">3D Object Detection (LiDAR + Camera)</th> <th style="text-align: center">Moving Object Segmentation</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><a href="https://youtu.be/TFJhpJ6JjMQ" rel="external nofollow noopener" target="_blank"><img src="../assets/img/PhDThesis/srfdet3d_ss.png" width="500" height="250" class="center"></a></td> <td style="text-align: center"><a href="https://youtu.be/4eYi80o8MQI" rel="external nofollow noopener" target="_blank"><img src="../assets/img/PhDThesis/mosnet_ss.png" width="500" height="250" class="center"></a></td> </tr> </tbody> </table> <hr> <h3 id="3d-object-detection-and-relative-localization-using-a-3d-sensor-embedded-on-a-mobile-robot">3D Object Detection and Relative Localization using a 3D Sensor Embedded on a Mobile Robot</h3> <p><a href="https://github.com/gopi-erabati/Object-Pose-Estimation" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Object-Pose-Estimation/blob/master/GopikrishnaErabati-thesis.pdf" rel="external nofollow noopener" target="_blank">Master Thesis</a> at LAAS-CNRS, Toulouse, France</p> <p>This thesis tries to solve the issue of object pose estimation using 3D data of scene acquired from 3D sensors (e.g., Kinect, Orbec Astra Pro among others). 3D data has an advantage of independence from object texture and invariance to illumination. The proposal is divided into two phases: An offline phase where the 3D model template of the object (for estimation of pose) is built using Iterative Closest Point (ICP) algorithm. And an online phase where the pose of the object is estimated by aligning the scene to the model using ICP, provided with an initial alignment using 3D descriptors (like Fast Point Feature Transform (FPFH)). The approach we develop is to be integrated on two different platforms: 1) Humanoid robot ‘Pyrene’ which has Orbec Astra Pro 3D sensor for data acquisition, and 2) Unmanned Aerial Vehicle (UAV) which has Intel RealSense Euclid on it. The datasets of objects (like electric drill, brick, a small cylinder, cake box) are acquired using Microsoft Kinect, Orbec Astra Pro and Intel RealSense Euclid sensors to test the performance of this technique. The objects used to test this approach are the ones used by robots. This technique is tested in two scenarios, firstly, when the object is on the table and secondly when the object is held in hand by a person. The range of objects from the sensor is 0.6 to 1.6m. This technique could handle occlusions of the object by hand (when we hold the object), as ICP can work even if partial object is visible in the scene.</p> <table> <thead> <tr> <th style="text-align: center">Single object pose estimation</th> <th style="text-align: center">Multiple objects pose estimation</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><a href="https://www.youtube.com/watch?v=8NoalZesWR0" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/objposeest1.png" width="500" height="250" class="center"></a></td> <td style="text-align: center"><a href="https://www.youtube.com/watch?v=cyvhGSBhMF0" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/objposeest2.png" width="500" height="250" class="center"></a></td> </tr> </tbody> </table> <hr> <h3 id="human-activity-recognition-in-video">Human Activity Recognition in Video</h3> <p><a href="https://github.com/gopi-erabati/Human-Activity-Recognition-from-Videos-Using-Machine-Learning" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Human-Activity-Recognition-from-Videos-Using-Machine-Learning/blob/master/report_HumanActivityDetction_SSI_GopikrishnaErabati_MohitKumarAhuja.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>Actions can be characterized by spatiotemporal patterns. Like object detection, action detection finds the recurrences of such spatiotemporal patterns through pattern matching. I worked on a few types of interest-point based feature extractions like Spatio-Temporal Interest Point (STIP), 3D SIFT and Histogram of Oriented Optical Flow (HOOF) features. With the use of SVM classifier, I classified the actions. I implemented this human activity recognition on KTH dataset which has six actions like boxing, hand waving, hand clapping, jogging, running, and walking of 100 videos each.</p> <p><a href="https://www.youtube.com/watch?v=Cn639T80BhQ" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/action.png" width="500" height="250"></a></p> <hr> <h3 id="mapping-autonomous-navigation-and-localization-of-turtlebot-using-ros">Mapping, Autonomous Navigation and Localization of TurtleBot using ROS</h3> <p><a href="https://github.com/gopi-erabati/Mapping_LocalizationOfARMarkers_Navigation_ROS_Turtlebot" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Mapping_LocalizationOfARMarkers_Navigation_ROS_Turtlebot/blob/master/report_mappingAndLocalization_ROS.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>The motto of the project is to gain experience in the implementation of different robotic algorithms using ROS framework.</p> <ol> <li>The first step of the task is to build a map of the environment and navigate to a desired location on the map.</li> <li>Next, we must sense the location of marker (e.g., AR marker, color markers etc.) in the map, where there is pick and place task, and autonomously localize and navigate to the desired marker location.</li> <li>After reaching the desired marker location, we have to precisely move towards the specified location based on visual servoing.</li> <li>At the desired location, we have a robotic arm which picks up an object (e.g., a small cube) and places it on our TurtleBot (called the pick and place task).</li> <li>After the pick and place task, again the robot needs to find another marker, which specifies the final target location, and autonomously localize and navigate to the desired marker location, which finishes the complete task of the project.</li> </ol> <p>Our team developed the software to implement the 1, 2 and 5 points in the above list. The implementation can be found in the attached project report.</p> <p><a href="https://www.youtube.com/watch?v=YPKXm8cRIKs" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/turtlebotros.png" width="500" height="250"></a></p> <hr> <h3 id="development-of-3d-human-body-scanner-using-kinect-and-pcl">Development of 3D Human Body Scanner using Kinect and PCL</h3> <p><a href="https://github.com/gopi-erabati/3D-Reconstruction-of-Human-using-Kinect-v2-and-PCL" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/3D-Reconstruction-of-Human-using-Kinect-v2-and-PCL" rel="external nofollow noopener" target="_blank">Report</a></p> <p>The main objective of this project is to develop human 3D scanner software able to fully interface with a scanner rig composed of a turning table and a stationary depth sensor. The software is aimed to perform full body scan under 90 seconds. A friendly, interactive graphical user interface provides simple control and outputs watertight mesh results that can be used mainly but not limited to 3D printing. The project was implemented using Microsoft Kinect v2 and PCL library in a Windows OS.</p> <table> <thead> <tr> <th style="text-align: center">Registration using ICP with Normals</th> <th style="text-align: center">Noise Removal and Smoothing</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><img src="../assets/img/projects/scanner1.png" alt=""></td> <td style="text-align: center"><img src="../assets/img/projects/scanner2.png" alt=""></td> </tr> </tbody> </table> <hr> <h3 id="development-of-computer-vision-toolbox-in-c-and-matlab-using-opencv">Development of Computer Vision Toolbox in C++ and MATLAB using OpenCV</h3> <p><a href="https://github.com/gopi-erabati/Development-of-Image-Processing-and-Computer-Vision-Toolbox-using-C-and-OpenCV" rel="external nofollow noopener" target="_blank">Code(C++)</a> <a href="https://github.com/gopi-erabati/Development-of-Image-Processing-and-Computer-Vision-Toolbox-using-C-and-OpenCV/blob/master/report_openCV_cPlusPlus_GoikrishnaErabati.pdf" rel="external nofollow noopener" target="_blank">Report(C++)</a> <a href="https://github.com/gopi-erabati/Development-of-Image-Processing-and-Computer-Vision-Toolbox-using-MATLAB" rel="external nofollow noopener" target="_blank">Code(MATLAB)</a> <a href="https://github.com/gopi-erabati/Development-of-Image-Processing-and-Computer-Vision-Toolbox-using-MATLAB/blob/master/Report_CVToolBox_Matlab_GoikrishnaErabati.pdf" rel="external nofollow noopener" target="_blank">Report(MATLAB)</a></p> <p>In this project I aimed at developing CV tools like feature matching, camera calibration etc. not only using OpenCV and C++ but also with MATLAB.</p> <table> <thead> <tr> <th style="text-align: center">CV Toolbox GUI in C++</th> <th style="text-align: center">CV Toolbox GUI in MATLAB</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><img src="../assets/img/projects/cvtoolbox1.png" width="500" height="250"></td> <td style="text-align: center"><img src="../assets/img/projects/cvtoolboxmatlab.png" width="500" height="250"></td> </tr> </tbody> </table> <p><a href="https://www.youtube.com/watch?v=9FG2c6Qe0XM" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/cvtoolboxss.png" width="500" height="250"></a></p> <hr> <h3 id="development-of-face-recognition-software-using-pca">Development of Face Recognition Software using PCA</h3> <p><a href="https://github.com/gopi-erabati/Face-Recognition-Using-PCA" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Face-Recognition-Using-PCA/blob/master/report_facerecog_PCA_ERABATI_DOUSAI.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>The main objective of our project is to recognize faces from the collected set of face data using PCA (Principal Component Analysis). We have collected five pictures of every person, and the idea here is to extract a few features from the faces with the goal of reducing the number of variables used to represent the faces. In the next step, we divide the faces into two categories of train and test images. But the problem here is, an image has high dimensionality space (each image is a point in a space of dimension d = MN, M and N being image size) as each pixel is considered as a variable of an image. So, we can reduce the dimensionality by using PCA to simplify recognition problem, which can be considered as the core concept.</p> <p><img src="../assets/img/projects/facerecog.png" width="500" height="250"></p> <hr> <h3 id="bug0-algorithm-implementation-on-e-puck-robot">Bug0 Algorithm Implementation on E-Puck Robot</h3> <p><a href="https://github.com/gopi-erabati/Bug0-Algorithm-Implementation-on-E-Puck-Robot" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Bug0-Algorithm-Implementation-on-E-Puck-Robot/blob/master/report_bug0.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>Bug 0 is a reactive-navigation algorithm that uses odometry. The robot starts with knowledge of the position of goal relative to its initial pose, but no knowledge of the environment. The strategy is then to use odometry to orient and move toward goal while reactively avoiding obstacles.</p> <table> <thead> <tr> <th style="text-align: center">Bug 0 simulation 1</th> <th style="text-align: center">Bug 0 simulation 2</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><a href="https://www.youtube.com/watch?v=-hTFp8UpcRo" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/bug0.png" width="500" height="250" class="center"></a></td> <td style="text-align: center"><a href="https://www.youtube.com/watch?v=R6BD9uD3BnU" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/bug0_1.png" width="500" height="250" class="center"></a></td> </tr> </tbody> </table> <hr> <h3 id="wall-follower-e-puck-robot">Wall Follower E-Puck Robot</h3> <p><a href="https://github.com/gopi-erabati/Wall-Follower-E-Puck-Robot" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Wall-Follower-E-Puck-Robot/blob/master/Ghimire_Erabati_P0report.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>Objectives in this project were: 1. Move the robot forward and stop at some distance from the wall and 2. Follow the encountered wall. Our approach was to build small behaviors like ‘move forward’, ‘stop’, etc. and organize them into a composite behavior that fulfilled the objectives. To make our robot follow a wall, we developed a ‘follow wall’ behavior based on PID control. We worked with a robot called e-puck, a differential-drive non-holonomic robot, to develop and deploy simple concepts in autonomous behavior-based robotics.</p> <table> <thead> <tr> <th style="text-align: center">Wall Follower (Simulation)</th> <th style="text-align: center">Wall Follower (Real-time)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><a href="https://www.youtube.com/watch?v=gT462ntTajk" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/wallfollow_sim.png" width="500" height="250" class="center"></a></td> <td style="text-align: center"><a href="https://www.youtube.com/watch?v=0B48A3jn3qA" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/wallfollow_real.png" width="500" height="250" class="center"></a></td> </tr> </tbody> </table> <hr> <h3 id="forest-fire-mapping-from-low-altitude-aerial-images">Forest Fire Mapping from Low Altitude Aerial Images</h3> <p><a href="https://github.com/gopi-erabati/Forest-Fire-Mapping-from-low-altitude-aerial-images" rel="external nofollow noopener" target="_blank">Code</a></p> <p>This project deals with the projection of forest fire segmented images onto the Digital Elevation Models (DEMs) using the Pinhole camera model. The project is developed in C++ using OpenCV and GDAL libraries. The inputs are: Digital Elevation Map (DEM) - raster model and Geo-referenced Images. The output is a fire map. This code concentrates on segmenting the geo-referenced images and updating the fire map by projecting the segmented images onto the DEM.</p> <hr> <h3 id="classification-using-svm">Classification using SVM</h3> <p><a href="https://github.com/gopi-erabati/Classification-of-cats-and-dogs-using-SVM-classifier" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Classification-of-cats-and-dogs-using-SVM-classifier/blob/master/report_hw2_GopikrishnaErabati.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>The project aims to classify dogs and cats with the help of feature descriptor for representing the training data and training the SVM and application of cross-validation.</p> <hr> <h3 id="build-a-classifier-to-filter-spam-emails">Build a Classifier to Filter Spam Emails</h3> <p><a href="https://github.com/gopi-erabati/Build-a-classifier-to-filter-spam-emails" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Build-a-classifier-to-filter-spam-emails/blob/master/report_hw2_GopikrishnaErabati.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>The aim of this project is to build a classifier to filter spam emails. Before training a classifier, we can apply several preprocessing methods to this data such as standardize, transform or binarize data. For each preprocessing, a logistic regression model is fitted and compared with Naive Bayes classifier.</p> <hr> <h3 id="projective-reconstruction">Projective Reconstruction</h3> <p><a href="https://github.com/gopi231091/Projective-Reconstruction" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Projective-Reconstruction/blob/master/report_GOPIKRISHNA-ERABATI.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>This project aims to understand the pin-hole camera model and projective geometry. It deals with the computation of the fundamental matrix from camera parameters and the estimation of the fundamental matrix using two images. This project provides insight into epipolar geometry and stereovision.</p> <hr> <h3 id="implementing-horn-schunck-and-lucas-kanade-optical-flow-methods">Implementing Horn-Schunck and Lucas Kanade Optical Flow Methods</h3> <p><a href="https://github.com/gopi-erabati/Implementing-Horn-Schunck-and-Lucas-Kanade-Optical-Flow-Methods" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Implementing-Horn-Schunck-and-Lucas-Kanade-Optical-Flow-Methods/blob/master/report_GOPIKRISHNA-ERABATI.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>The pattern of apparent motion of objects, surfaces, and edges in a visual scene that results from the relative motion between an observer and a scene is known as optical flow. It depends on the brightness constancy assumption. Horn and Schunck method is a global method to find optical flow. The main idea is to make the optical flow smooth. As the Optical Flow Constraint Equation (OFCE) is an under constrained equation that cannot be solved for each pixel, they proposed to add another smoothness term to OFCE. This method works for small motion. Lucas Kanade is a local method. The main idea is that the optical flow is constant near the current point (x, y). Each neighbor gives one equation. Here we assume that pixel’s neighbor has same velocity (u, v). by taking pixels in a neighborhood, we get an over determined system which can be solved by Linear least squares or by pseudo inverse.</p> <hr> <h3 id="2d-filtering-using-vhdl-and-fpga">2D Filtering using VHDL and FPGA</h3> <p><a href="https://github.com/gopi-erabati/2D-Filtering-using-VHDL-and-FPGA" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/2D-Filtering-using-VHDL-and-FPGA/blob/master/report_2DFilterImple_GopikrishnaERABATI_AvinashNARAYANA.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>The task is to implement a 2D filter to process the images using FPGA and VHDL (Very High-Speed Integrated Circuit Hardware Description Language). The image is 128 x 128 pixels in resolution and the kernel size is 33 pixels. The software used for the simulation and implementation is Xilinx ISE (Integrated Synthesis Environment) Design Suite and the language is VHDL.</p> <hr> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Gopi Krishna Erabati. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: July 20, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?fa0110e8b42cec56ce96d912fd4bde74"></script> <script>addBackToTop();</script> </body> </html>