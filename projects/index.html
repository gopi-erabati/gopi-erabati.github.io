<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Projects | Gopi Krishna Erabati </title> <meta name="author" content="Gopi Krishna Erabati"> <meta name="description" content="A PhD candidate at Institute of Systems and Robotics, University of Coimbra, Portugal, focusing on Scene Understanding for Autonomous Driving. "> <meta name="keywords" content="Computer Vision, Autonomous Driving, Deel Learning, Scene Understanding"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/6.jpg?b2748a8c220f6715934bfddac3f6df51"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gopi-erabati.github.io/projects/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Gopi Krishna</span> Erabati </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/experience/">Experience </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">Awards </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_Gopi.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Projects</h1> <p class="post-description"></p> </header> <article> <h2 id="research-projects">Research Projects</h2> <h3 id="3d-object-detection-and-relative-localization-using-a-3d-sensor-embedded-on-a-mobile-robot">3D Object Detection and Relative Localization using a 3D sensor embedded on a Mobile Robot</h3> <p><a href="https://github.com/gopi-erabati/Object-Pose-Estimation" rel="external nofollow noopener" target="_blank">Code</a>; <a href="https://github.com/gopi-erabati/Object-Pose-Estimation/blob/master/GopikrishnaErabati-thesis.pdf" rel="external nofollow noopener" target="_blank">Master Thesis</a> at LAAS-CNRS, Toulouse, France</p> <p>This thesis tries to solve the issue of object pose estimation using 3D data of scene acquired from 3D sensors (e.g., Kinect, Orbec Astra Pro among others). 3D data has an advantage of independence from object texture and invariance to illumination. The proposal is divided into two phases: An offline phase where the 3D model template of the object (for estimation of pose) is built using Iterative Closest Point (ICP) algorithm. And an online phase where the pose of the object is estimated by aligning the scene to the model using ICP, provided with an initial alignment using 3D descriptors (like Fast Point Feature Transform (FPFH)). The approach we develop is to be integrated on two different platforms: 1) Humanoid robot ‘Pyrene’ which has Orbec Astra Pro 3D sensor for data acquisition, and 2) Unmanned Aerial Vehicle (UAV) which has Intel RealSense Euclid on it. The datasets of objects (like electric drill, brick, a small cylinder, cake box) are acquired using Microsoft Kinect, Orbec Astra Pro and Intel RealSense Euclid sensors to test the performance of this technique. The objects used to test this approach are the ones used by robots. This technique is tested in two scenarios, firstly, when the object is on the table and secondly when the object is held in hand by a person. The range of objects from the sensor is 0.6 to 1.6m. This technique could handle occlusions of the object by hand (when we hold the object), as ICP can work even if partial object is visible in the scene.</p> <table> <thead> <tr> <th>Single object pose estimation</th> <th>Multiple objects pose estimation</th> </tr> </thead> <tbody> <tr> <td><a href="https://www.youtube.com/watch?v=8NoalZesWR0" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/objposeest1.png" alt=""></a></td> <td><a href="https://www.youtube.com/watch?v=cyvhGSBhMF0" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/objposeest2.png" alt=""></a></td> </tr> </tbody> </table> <hr> <h3 id="human-activity-recognition-in-video">Human Activity Recognition in Video</h3> <p><a href="https://github.com/gopi-erabati/Human-Activity-Recognition-from-Videos-Using-Machine-Learning" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Human-Activity-Recognition-from-Videos-Using-Machine-Learning/blob/master/report_HumanActivityDetction_SSI_GopikrishnaErabati_MohitKumarAhuja.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>Actions can be characterized by spatiotemporal patterns. Like object detection, action detection finds the recurrences of such spatiotemporal patterns through pattern matching. I worked on a few types of interest-point based feature extractions like Spatio-Temporal Interest Point (STIP), 3D SIFT and Histogram of Oriented Optical Flow (HOOF) features. With the use of SVM classifier, I classified the actions. I implemented this human activity recognition on KTH dataset which has six actions like boxing, hand waving, hand clapping, jogging, running, and walking of 100 videos each.</p> <p><a href="https://www.youtube.com/watch?v=Cn639T80BhQ" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/action.png" width="500" height="250"></a></p> <hr> <h3 id="mapping-autonomous-navigation-and-localization-of-turtlebot-using-ros">Mapping, Autonomous Navigation and Localization of TurtleBot using ROS</h3> <p><a href="https://github.com/gopi-erabati/Mapping_LocalizationOfARMarkers_Navigation_ROS_Turtlebot" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Mapping_LocalizationOfARMarkers_Navigation_ROS_Turtlebot/blob/master/report_mappingAndLocalization_ROS.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>The motto of the project is to gain experience in the implementation of different robotic algorithms using ROS framework.</p> <ol> <li>The first step of the task is to build a map of the environment and navigate to a desired location on the map.</li> <li>Next, we must sense the location of marker (e.g., AR marker, color markers etc.) in the map, where there is pick and place task, and autonomously localize and navigate to the desired marker location.</li> <li>After reaching the desired marker location, we have to precisely move towards the specified location based on visual servoing.</li> <li>At the desired location, we have a robotic arm which picks up an object (e.g., a small cube) and places it on our TurtleBot (called the pick and place task).</li> <li>After the pick and place task, again the robot needs to find another marker, which specifies the final target location, and autonomously localize and navigate to the desired marker location, which finishes the complete task of the project.</li> </ol> <p>Our team developed the software to implement the 1, 2 and 5 points in the above list. The implementation can be found in the attached project report.</p> <p><a href="https://www.youtube.com/watch?v=YPKXm8cRIKs" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/turtlebotros.png" width="500" height="250"></a></p> <hr> <h3 id="development-of-3d-human-body-scanner-using-kinect-and-pcl">Development of 3D Human Body Scanner using Kinect and PCL</h3> <p><a href="https://github.com/gopi-erabati/3D-Reconstruction-of-Human-using-Kinect-v2-and-PCL" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/3D-Reconstruction-of-Human-using-Kinect-v2-and-PCL" rel="external nofollow noopener" target="_blank">Report</a></p> <p>The main objective of this project is to develop human 3D scanner software able to fully interface with a scanner rig composed of a turning table and a stationary depth sensor. The software is aimed to perform full body scan under 90 seconds. A friendly, interactive graphical user interface provides simple control and outputs watertight mesh results that can be used mainly but not limited to 3D printing. The project was implemented using Microsoft Kinect v2 and PCL library in a Windows OS.</p> <table> <thead> <tr> <th>Registration using ICP with Normals</th> <th>Noise Removal and Smoothing</th> </tr> </thead> <tbody> <tr> <td><img src="../assets/img/projects/scanner1.png" alt=""></td> <td><img src="../assets/img/projects/scanner2.png" alt=""></td> </tr> </tbody> </table> <hr> <h3 id="development-of-computer-vision-toolbox-in-c-and-matlab-using-opencv">Development of Computer Vision Toolbox in C++ and MATLAB using OpenCV</h3> <p><a href="https://github.com/gopi-erabati/Development-of-Image-Processing-and-Computer-Vision-Toolbox-using-C-and-OpenCV" rel="external nofollow noopener" target="_blank">Code(C++)</a> <a href="https://github.com/gopi-erabati/Development-of-Image-Processing-and-Computer-Vision-Toolbox-using-C-and-OpenCV/blob/master/report_openCV_cPlusPlus_GoikrishnaErabati.pdf" rel="external nofollow noopener" target="_blank">Report(C++)</a> <a href="https://github.com/gopi-erabati/Development-of-Image-Processing-and-Computer-Vision-Toolbox-using-MATLAB" rel="external nofollow noopener" target="_blank">Code(MATLAB)</a> <a href="https://github.com/gopi-erabati/Development-of-Image-Processing-and-Computer-Vision-Toolbox-using-MATLAB/blob/master/Report_CVToolBox_Matlab_GoikrishnaErabati.pdf" rel="external nofollow noopener" target="_blank">Report(MATLAB)</a></p> <p>In this project I aimed at developing CV tools like feature matching, camera calibration etc. not only using OpenCV and C++ but also with MATLAB.</p> <table> <thead> <tr> <th>CV Toolbox GUI in C++</th> <th>CV Toolbox GUI in MATLAB</th> </tr> </thead> <tbody> <tr> <td><img src="../assets/img/projects/cvtoolbox1.png" alt=""></td> <td><img src="../assets/img/projects/cvtoolboxmatlab.png" alt=""></td> </tr> </tbody> </table> <p><a href="https://www.youtube.com/watch?v=9FG2c6Qe0XM" rel="external nofollow noopener" target="_blank"><img src="../assets/img/projects/cvtoolboxss.png" width="500" height="250"></a></p> <hr> <h3 id="development-of-face-recognition-software-using-pca">Development of Face Recognition software using PCA</h3> <p><a href="https://github.com/gopi-erabati/Face-Recognition-Using-PCA" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/gopi-erabati/Face-Recognition-Using-PCA/blob/master/report_facerecog_PCA_ERABATI_DOUSAI.pdf" rel="external nofollow noopener" target="_blank">Report</a></p> <p>The main objective of our project is to recognize faces from the collected set of face data using PCA (Principal Component Analysis). We have collected five pictures of every person, and the idea here is to extract a few features from the faces with the goal of reducing the number of variables used to represent the faces. In the next step, we divide the faces into two categories of train and test images. But the problem here is, an image has high dimensionality space (each image is a point in a space of dimension d = MN, M and N being image size) as each pixel is considered as a variable of an image. So, we can reduce the dimensionality by using PCA to simplify recognition problem, which can be considered as the core concept.</p> <p><img src="../assets/img/projects/facerecog.png" width="500" height="250"></p> <hr> <p>###</p> <ul> <li>Human Activity Recognition in Videos. [<a href="https://github.com/gopi-erabati/Human-Activity-Recognition-from-Videos-Using-Machine-Learning" rel="external nofollow noopener" target="_blank">Code</a>]</li> <li>Mapping, Autonomous Navigation and Localization of Turtlebot using ROS. [<a href="https://github.com/gopi-erabati/Mapping_LocalizationOfARMarkers_Navigation_ROS_Turtlebot" rel="external nofollow noopener" target="_blank">Code</a>]</li> <li>Development of Computer Vision Toolbox in C++ [<a href="https://github.com/gopi-erabati/Development-of-Image-Processing-and-Computer-Vision-Toolbox-using-C-and-OpenCV" rel="external nofollow noopener" target="_blank">Code</a>] and MATLAB [<a href="https://github.com/gopi-erabati/Development-of-Image-Processing-and-Computer-Vision-Toolbox-using-MATLAB" rel="external nofollow noopener" target="_blank">Code</a>] using OpenCV.</li> <li>Development of 3D Scanner using Kinect and PCL. [<a href="https://github.com/gopi-erabati/3D-Reconstruction-of-Human-using-Kinect-v2-and-PCL" rel="external nofollow noopener" target="_blank">Code</a>]</li> <li>Development of Face Recognition software using PCA. [<a href="https://github.com/gopi-erabati/Face-Recognition-Using-PCA" rel="external nofollow noopener" target="_blank">Code</a>]</li> <li>Classification using SVM. [<a href="https://github.com/gopi-erabati/Classification-of-cats-and-dogs-using-SVM-classifier" rel="external nofollow noopener" target="_blank">Code</a>]</li> <li>Click <a href="https://www.linkedin.com/in/gopierabati/details/projects/" rel="external nofollow noopener" target="_blank">here</a> for more projects</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Gopi Krishna Erabati. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: July 20, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?fa0110e8b42cec56ce96d912fd4bde74"></script> <script>addBackToTop();</script> </body> </html>